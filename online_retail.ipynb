{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Project: Online Retail EDA with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As an entry-level data analyst, I’m exploring transactional data from an online retail company to uncover insights that can drive better business decisions. This dataset includes customer purchases, product details, quantities, prices, and timestamps.  \n",
    "\n",
    "\n",
    "### Objectives  \n",
    "- Perform **Exploratory Data Analysis (EDA)** to identify key trends.  \n",
    "- Analyze **sales performance, customer behavior, and popular products**.  \n",
    "- Provide **data-driven recommendations** to optimize online retail strategies.  \n",
    "\n",
    "\n",
    "### My Approach  \n",
    "To tackle this project, I’ll start by **ETL (Extract, Transform, Load)** to clean and prepare the dataset. Then, I’ll conduct in-depth analysis to identify key trends and insights like **busiest sales periods, top-selling products, and high-value customers**. Let's dive in!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this project, I'll be working with the **Online Retail** dataset, which contains transactional data from an online store between 2010 and 2011. The dataset is in a `.csv` file named **`online_retail.csv`**, and it includes details about purchases such as product descriptions, quantities, prices, timestamps, and customer IDs.  \n",
    "\n",
    "\n",
    "### Data Columns  \n",
    "The dataset consists of the following fields:  \n",
    "- **InvoiceNo** – Unique invoice number for each transaction.  \n",
    "- **StockCode** – Unique product identifier.  \n",
    "- **Description** – Product name/description.  \n",
    "- **Quantity** – Number of units purchased.  \n",
    "- **InvoiceDate** – Timestamp of the transaction.  \n",
    "- **UnitPrice** – Price per unit of the product.  \n",
    "- **CustomerID** – Unique identifier for each customer.  \n",
    "- **Country** – Country where the transaction took place.  \n",
    "\n",
    "\n",
    "### My Approach  \n",
    "\n",
    "To analyze this dataset effectively, I’ll break the process into key steps:  \n",
    "\n",
    "1. **Load the data** into a Pandas DataFrame and inspect the first few rows.  \n",
    "2. **Clean the dataset** by handling missing values and removing unnecessary columns.  \n",
    "3. **Explore basic statistics** to understand distributions and trends.  \n",
    "4. **Visualize the data** using plots such as histograms, bar charts, and scatter plots.  \n",
    "5. **Analyze sales trends** over time to identify peak sales periods.  \n",
    "6. **Identify top-selling products and countries** based on quantity sold.  \n",
    "7. **Detect anomalies or outliers** that may impact the analysis.  \n",
    "8. **Summarize key findings** and insights from the data.  \n",
    "\n",
    "Let's dive in and explore the dataset!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"source/online_retail.csv\", encoding=\"ISO-8859-1\")  # We use encoding to avoid UnicodeDecodeError (or encoding=\"Windows-1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore and familiarize with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>541909.000000</td>\n",
       "      <td>541909.000000</td>\n",
       "      <td>406829.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.552250</td>\n",
       "      <td>4.611114</td>\n",
       "      <td>15287.690570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>218.081158</td>\n",
       "      <td>96.759853</td>\n",
       "      <td>1713.600303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-80995.000000</td>\n",
       "      <td>-11062.060000</td>\n",
       "      <td>12346.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>13953.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>15152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.130000</td>\n",
       "      <td>16791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80995.000000</td>\n",
       "      <td>38970.000000</td>\n",
       "      <td>18287.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Quantity      UnitPrice     CustomerID\n",
       "count  541909.000000  541909.000000  406829.000000\n",
       "mean        9.552250       4.611114   15287.690570\n",
       "std       218.081158      96.759853    1713.600303\n",
       "min    -80995.000000  -11062.060000   12346.000000\n",
       "25%         1.000000       1.250000   13953.000000\n",
       "50%         3.000000       2.080000   15152.000000\n",
       "75%        10.000000       4.130000   16791.000000\n",
       "max     80995.000000   38970.000000   18287.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n",
    "\n",
    "# TODO: Found some negative values in quantity and unit price. We need to check if they are valid or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the data types of each column and detected any missing (null) values, we have a clearer understanding of how to approach the ETL process.\n",
    "\n",
    "Before proceeding, let's create a copy of the dataframe to preserve the original data in its unaltered state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the copy created, we will begin by modifying the data types of specific columns.  \n",
    "In this case, we will convert the `Country`, `InvoiceNo`, and `StockCode` columns from the object type to the category type.  \n",
    "This transformation will optimize memory usage and improve performance when handling these columns in Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Country'] = df['Country'].astype('category')\n",
    "df_clean['InvoiceNo'] = df['InvoiceNo'].astype('category')\n",
    "df_clean['StockCode'] = df['StockCode'].astype('category')\n",
    "\n",
    "# Ensure the data types where set correctly with: df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing (null) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing `CustomerID` values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains missing values in the `CustomerID` column, but these transactions are still valid purchases. Instead of dropping them or imputing arbitrary values (which could introduce bias), I will leave them as `NaN`.\n",
    "\n",
    " Why?\n",
    "- Removing these rows would result in **loss of actual transaction data**.\n",
    "- Imputing fake IDs would be **misleading**, as customer IDs are unique identifiers.\n",
    "- Pandas and Matplotlib **handle NaN values gracefully** in most operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing `Description` values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains null values in the `Description` column. Since these rows cannot be dropped without losing valuable data, we impute the missing descriptions using the corresponding `StockCode` values (which are complete and unique).\n",
    "\n",
    "For that purpose, we follow this steps:\n",
    "1. **Create a mapping dictionary** where each `StockCode` points to its correct `Description` (using only rows with non-null descriptions)\n",
    "2. **Fill null values** by matching each missing `Description` with its `StockCode`'s known description\n",
    "\n",
    "**Key Note**: If a `StockCode` has no valid description in the dataset, its `NaN` values will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Map StockCode to Description (drop duplicates to ensure 1:1 mapping)\n",
    "stock_to_desc = df_clean.dropna(subset=['Description']).drop_duplicates('StockCode').set_index('StockCode')['Description']\n",
    "\n",
    "# Step 2: Fill NaN Descriptions using the mapped StockCode values\n",
    "df_clean['Description'] = df_clean['Description'].fillna(df_clean['StockCode'].map(stock_to_desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After handling the preliminary missing values in the `Description` column, it's important to verify if any null values still remain. We will perform this check to ensure that all missing descriptions have been properly handled before moving forward with further analysis.\n",
    "\n",
    "To do so, we'll check for any remaining nulls in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Description column null values: 1454\n",
      "Updated Description column null values: 112\n"
     ]
    }
   ],
   "source": [
    "# This will give us an updated count of the missing values in the 'Description' column\n",
    "print(f'Original Description column null values: {df['Description'].isna().sum()}')\n",
    "print(f'Updated Description column null values: {df_clean['Description'].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing Remaining Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking for null values, we found that 112 missing descriptions remain out of the initial 1,454 null values. To ensure we don't lose valuable transaction data, we will impute these remaining null values with the placeholder `'Unknown'`. This decision allows us to retain all rows in the dataset while clearly marking the transactions with missing descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Description'] = df_clean['Description'].fillna('Unknown')\n",
    "\n",
    "# To make sure this worked as intended: print(df_clean['Description'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, we preserve the full dataset while handling missing descriptions in a way that keeps the integrity of our analysis intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling negative invalid data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
