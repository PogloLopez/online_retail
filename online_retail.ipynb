{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio Project: Online Retail EDA with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project explores an **online retail transactions dataset**, focusing on **data cleaning, exploratory data analysis (EDA), and deriving business insights**. The dataset contains information about customer purchases, including invoice details, product descriptions, quantities, prices, and customer IDs. \n",
    "\n",
    "\n",
    "### Objectives  \n",
    "- Perform **Exploratory Data Analysis (EDA)** to identify key trends.  \n",
    "- Analyze **sales performance, customer behavior, and popular products**.  \n",
    "- Provide **data-driven recommendations** to optimize online retail strategies.  \n",
    "\n",
    "\n",
    "### My Approach  \n",
    "To tackle this project, I’ll start by **ETL (Extract, Transform, Load)** to clean and prepare the dataset. Then, I’ll conduct in-depth analysis to identify key trends and insights like **busiest sales periods, top-selling products, and high-value customers**. Let's dive in!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this project, I'll be working with the **Online Retail** dataset, which contains transactional data from an online store between 2010 and 2011. The dataset is in a `.csv` file named **`online_retail.csv`**, and it includes details about purchases such as product descriptions, quantities, prices, timestamps, and customer IDs.  \n",
    "\n",
    "\n",
    "### Data Columns  \n",
    "The dataset consists of the following fields:  \n",
    "- **InvoiceNo** – Unique invoice number for each transaction.  \n",
    "- **StockCode** – Unique product identifier.  \n",
    "- **Description** – Product name/description.  \n",
    "- **Quantity** – Number of units purchased.  \n",
    "- **InvoiceDate** – Timestamp of the transaction.  \n",
    "- **UnitPrice** – Price per unit of the product.  \n",
    "- **CustomerID** – Unique identifier for each customer.  \n",
    "- **Country** – Country where the transaction took place.  \n",
    "\n",
    "\n",
    "### My Approach  \n",
    "\n",
    "To analyze this dataset effectively, I’ll break the process into key steps:  \n",
    "\n",
    "1. **Load the data** into a Pandas DataFrame and inspect the first few rows.  \n",
    "2. **Clean the dataset** by handling missing values and removing unnecessary columns.  \n",
    "3. **Explore basic statistics** to understand distributions and trends.  \n",
    "4. **Visualize the data** using plots such as histograms, bar charts, and scatter plots.  \n",
    "5. **Analyze sales trends** over time to identify peak sales periods.  \n",
    "6. **Identify top-selling products and countries** based on quantity sold.  \n",
    "7. **Detect anomalies or outliers** that may impact the analysis.  \n",
    "8. **Summarize key findings** and insights from the data.  \n",
    "\n",
    "Let's dive in and explore the dataset!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"source/online_retail.csv\", encoding=\"ISO-8859-1\")  # We use encoding to avoid UnicodeDecodeError (or encoding=\"Windows-1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore and familiarize with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   InvoiceNo    541909 non-null  object \n",
      " 1   StockCode    541909 non-null  object \n",
      " 2   Description  540455 non-null  object \n",
      " 3   Quantity     541909 non-null  int64  \n",
      " 4   InvoiceDate  541909 non-null  object \n",
      " 5   UnitPrice    541909 non-null  float64\n",
      " 6   CustomerID   406829 non-null  float64\n",
      " 7   Country      541909 non-null  object \n",
      "dtypes: float64(2), int64(1), object(5)\n",
      "memory usage: 33.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quantity</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>541909.000000</td>\n",
       "      <td>541909.000000</td>\n",
       "      <td>406829.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.552250</td>\n",
       "      <td>4.611114</td>\n",
       "      <td>15287.690570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>218.081158</td>\n",
       "      <td>96.759853</td>\n",
       "      <td>1713.600303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-80995.000000</td>\n",
       "      <td>-11062.060000</td>\n",
       "      <td>12346.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>13953.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.080000</td>\n",
       "      <td>15152.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.130000</td>\n",
       "      <td>16791.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80995.000000</td>\n",
       "      <td>38970.000000</td>\n",
       "      <td>18287.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Quantity      UnitPrice     CustomerID\n",
       "count  541909.000000  541909.000000  406829.000000\n",
       "mean        9.552250       4.611114   15287.690570\n",
       "std       218.081158      96.759853    1713.600303\n",
       "min    -80995.000000  -11062.060000   12346.000000\n",
       "25%         1.000000       1.250000   13953.000000\n",
       "50%         3.000000       2.080000   15152.000000\n",
       "75%        10.000000       4.130000   16791.000000\n",
       "max     80995.000000   38970.000000   18287.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()\n",
    "\n",
    "# TODO: Found some negative values in quantity and unit price. We need to check if they are valid or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. Clean the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified the data types of each column and detected any missing (null) values, we have a clearer understanding of how to approach the ETL process.\n",
    "\n",
    "Before proceeding, let's create a copy of the dataframe to preserve the original data in its unaltered state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the copy created, we will begin by modifying the data types of specific columns.  \n",
    "In this case, we will convert the `Country`, `InvoiceNo`, and `StockCode` columns from the object type to the category type.  \n",
    "This transformation will optimize memory usage and improve performance when handling these columns in Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Country'] = df['Country'].astype('category')\n",
    "df_clean['InvoiceNo'] = df['InvoiceNo'].astype('category')\n",
    "df_clean['StockCode'] = df['StockCode'].astype('category')\n",
    "\n",
    "# Ensure the data types where set correctly with: df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing `CustomerID` values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains missing values in the `CustomerID` column, but these transactions are still valid purchases. Instead of dropping them or imputing arbitrary values (which could introduce bias), I will leave them as `NaN`.\n",
    "\n",
    " Why?\n",
    "- Removing these rows would result in **loss of actual transaction data**.\n",
    "- Imputing fake IDs would be **misleading**, as customer IDs are unique identifiers.\n",
    "- Pandas and Matplotlib **handle NaN values gracefully** in most operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling missing `Description` values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains null values in the `Description` column. Since these rows cannot be dropped without losing valuable data, we impute the missing descriptions using the corresponding `StockCode` values (which are complete and unique).\n",
    "\n",
    "For that purpose, we follow this steps:\n",
    "1. **Create a mapping dictionary** where each `StockCode` points to its correct `Description` (using only rows with non-null descriptions)\n",
    "2. **Fill null values** by matching each missing `Description` with its `StockCode`'s known description\n",
    "\n",
    "**Key Note**: If a `StockCode` has no valid description in the dataset, its `NaN` values will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Map StockCode to Description (drop duplicates to ensure 1:1 mapping)\n",
    "stock_to_desc = df_clean.dropna(subset=['Description']).drop_duplicates('StockCode').set_index('StockCode')['Description']\n",
    "\n",
    "# Step 2: Fill NaN Descriptions using the mapped StockCode values\n",
    "df_clean['Description'] = df_clean['Description'].fillna(df_clean['StockCode'].map(stock_to_desc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After handling the preliminary missing values in the `Description` column, it's important to verify if any null values still remain. We will perform this check to ensure that all missing descriptions have been properly handled before moving forward with further analysis.\n",
    "\n",
    "To do so, we'll check for any remaining nulls in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Description column null values: 1454\n",
      "Updated Description column null values: 112\n"
     ]
    }
   ],
   "source": [
    "# This will give us an updated count of the missing values in the 'Description' column\n",
    "print(f'Original Description column null values: {df['Description'].isna().sum()}')\n",
    "print(f'Updated Description column null values: {df_clean['Description'].isna().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing Remaining Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking for null values, we found that 112 missing descriptions remain out of the initial 1,454 null values. To ensure we don't lose valuable transaction data, we will impute these remaining null values with the placeholder `'Unknown'`. This decision allows us to retain all rows in the dataset while clearly marking the transactions with missing descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['Description'] = df_clean['Description'].fillna('Unknown')\n",
    "\n",
    "# To make sure this worked as intended: print(df_clean['Description'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this, we preserve the full dataset while handling missing descriptions in a way that keeps the integrity of our analysis intact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Processing\n",
    "\n",
    "From here forward, I need to write better the markdown documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we intend to remove duplicates in the next step, we first need to make sure that Pandas recognizes all duplicated rows as so. For that, we'll make sure to that all the strings are consistent and trimmed. \n",
    "\n",
    "For that purpose, we'll manage mainly 2 columns: `Description` and `Country`, since they are the only ones with string type data. \n",
    "For `Description`, we'll trim the leading, trailing, and in-between extra whitespaces, and standarize it to all uppercase\n",
    "For `Country`, we'll also trim the leading, trailing, and in-between extra whitespaces, and standarize them all as titles, with the first letter as uppercase, and the rest as lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Description: Remove leading/trailing spaces, handle in-between extra spaces, and standardize to lowercase\n",
    "df_clean['Description'] = df_clean['Description'].str.strip().str.replace(r'\\s+', ' ', regex=True).str.upper()\n",
    "\n",
    "# Clean Country: Remove leading/trailing spaces, handle in-between extra spaces, and title-case the country names\n",
    "df_clean['Country'] = df_clean['Country'].str.strip().str.replace(r'\\s+', ' ', regex=True).str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any duplicated values may generate bias and result in false insights drawn from the dataset, so it's necessary to deal with them. For that, we start by checking if there is any duplicated data to handle.\n",
    "\n",
    "Since all the columns can have duplicated values individually, we just need to drop the rows that have duplicate values in all the columns at once. So we can easily use Pandas .drop_duplicates() method to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 5268\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of duplicate rows: {df_clean.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove exact duplicate rows\n",
    "df_clean = df_clean.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating Negative Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of df.describe() in the 'Load the data' step, we found some negative values in the `Quantity` and `UnitPrice` columns. Since they are not supposed to be there, we'll handle them by..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10587\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Check for negative values in Quantity and UnitPrice\n",
    "print(df_clean[df_clean['Quantity'] < 0].shape[0])\n",
    "print(df_clean[df_clean['UnitPrice'] < 0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `Quantity` negative values are substantially higher, we'll start by handling them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizing the negative values in `Quantity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206139</th>\n",
       "      <td>C554905</td>\n",
       "      <td>15058A</td>\n",
       "      <td>BLUE POLKADOT GARDEN PARASOL</td>\n",
       "      <td>-2</td>\n",
       "      <td>5/27/2011 11:17</td>\n",
       "      <td>7.95</td>\n",
       "      <td>14191.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466258</th>\n",
       "      <td>C576324</td>\n",
       "      <td>23470</td>\n",
       "      <td>CARD HOLDER LOVE BIRD LARGE</td>\n",
       "      <td>-1</td>\n",
       "      <td>11/14/2011 15:10</td>\n",
       "      <td>6.25</td>\n",
       "      <td>16161.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349892</th>\n",
       "      <td>C567543</td>\n",
       "      <td>23346</td>\n",
       "      <td>SPACEBOY BEAKER</td>\n",
       "      <td>-6</td>\n",
       "      <td>9/21/2011 10:19</td>\n",
       "      <td>1.25</td>\n",
       "      <td>13418.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221643</th>\n",
       "      <td>556260</td>\n",
       "      <td>37340</td>\n",
       "      <td>SOLD AS SET ON DOTCOM</td>\n",
       "      <td>-18</td>\n",
       "      <td>6/9/2011 18:02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176843</th>\n",
       "      <td>C552029</td>\n",
       "      <td>20982</td>\n",
       "      <td>12 PENCILS TALL TUBE SKULLS</td>\n",
       "      <td>-1</td>\n",
       "      <td>5/5/2011 18:11</td>\n",
       "      <td>0.85</td>\n",
       "      <td>15622.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72926</th>\n",
       "      <td>C542273</td>\n",
       "      <td>22099</td>\n",
       "      <td>CARAVAN SQUARE TISSUE BOX</td>\n",
       "      <td>-9</td>\n",
       "      <td>1/27/2011 10:01</td>\n",
       "      <td>1.25</td>\n",
       "      <td>12578.0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20393</th>\n",
       "      <td>538042</td>\n",
       "      <td>21763</td>\n",
       "      <td>VINTAGE WOODEN BAR STOOL</td>\n",
       "      <td>-4</td>\n",
       "      <td>12/9/2010 13:10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467904</th>\n",
       "      <td>C576375</td>\n",
       "      <td>22583</td>\n",
       "      <td>PACK OF 6 HANDBAG GIFT BOXES</td>\n",
       "      <td>-1</td>\n",
       "      <td>11/15/2011 8:52</td>\n",
       "      <td>2.55</td>\n",
       "      <td>16878.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437426</th>\n",
       "      <td>C574291</td>\n",
       "      <td>22633</td>\n",
       "      <td>HAND WARMER UNION JACK</td>\n",
       "      <td>-1</td>\n",
       "      <td>11/3/2011 15:19</td>\n",
       "      <td>2.10</td>\n",
       "      <td>15290.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79477</th>\n",
       "      <td>C542991</td>\n",
       "      <td>85019C</td>\n",
       "      <td>CURIOUS IMAGES NOTEBOOK SET</td>\n",
       "      <td>-12</td>\n",
       "      <td>2/2/2011 11:54</td>\n",
       "      <td>4.25</td>\n",
       "      <td>14460.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21473</th>\n",
       "      <td>C538085</td>\n",
       "      <td>82483</td>\n",
       "      <td>WOOD 2 DRAWER CABINET WHITE FINISH</td>\n",
       "      <td>-1</td>\n",
       "      <td>12/9/2010 14:43</td>\n",
       "      <td>5.95</td>\n",
       "      <td>16350.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225302</th>\n",
       "      <td>C556647</td>\n",
       "      <td>23081</td>\n",
       "      <td>GREEN METAL BOX ARMY SUPPLIES</td>\n",
       "      <td>-2</td>\n",
       "      <td>6/13/2011 16:14</td>\n",
       "      <td>8.25</td>\n",
       "      <td>13012.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198245</th>\n",
       "      <td>554023</td>\n",
       "      <td>82613B</td>\n",
       "      <td>METAL SIGN,CUPCAKE SINGLE HOOK</td>\n",
       "      <td>-71</td>\n",
       "      <td>5/20/2011 13:20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193448</th>\n",
       "      <td>C553530</td>\n",
       "      <td>22646</td>\n",
       "      <td>CERAMIC STRAWBERRY CAKE MONEY BANK</td>\n",
       "      <td>-5</td>\n",
       "      <td>5/17/2011 15:08</td>\n",
       "      <td>1.24</td>\n",
       "      <td>13777.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66000</th>\n",
       "      <td>C541710</td>\n",
       "      <td>21880</td>\n",
       "      <td>RED RETROSPOT TAPE</td>\n",
       "      <td>-12</td>\n",
       "      <td>1/21/2011 11:17</td>\n",
       "      <td>0.65</td>\n",
       "      <td>14320.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422036</th>\n",
       "      <td>C573037</td>\n",
       "      <td>23197</td>\n",
       "      <td>SKETCHBOOK MAGNETIC SHOPPING LIST</td>\n",
       "      <td>-6</td>\n",
       "      <td>10/27/2011 13:45</td>\n",
       "      <td>1.45</td>\n",
       "      <td>12471.0</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419246</th>\n",
       "      <td>C572761</td>\n",
       "      <td>22960</td>\n",
       "      <td>JAM MAKING SET WITH JARS</td>\n",
       "      <td>-3</td>\n",
       "      <td>10/26/2011 9:38</td>\n",
       "      <td>4.25</td>\n",
       "      <td>12584.0</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516815</th>\n",
       "      <td>C579970</td>\n",
       "      <td>22840</td>\n",
       "      <td>ROUND CAKE TIN VINTAGE RED</td>\n",
       "      <td>-2</td>\n",
       "      <td>12/1/2011 11:13</td>\n",
       "      <td>7.95</td>\n",
       "      <td>14388.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88062</th>\n",
       "      <td>C543755</td>\n",
       "      <td>21915</td>\n",
       "      <td>RED HARMONICA IN BOX</td>\n",
       "      <td>-12</td>\n",
       "      <td>2/11/2011 14:48</td>\n",
       "      <td>1.25</td>\n",
       "      <td>14304.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369950</th>\n",
       "      <td>C569114</td>\n",
       "      <td>23171</td>\n",
       "      <td>REGENCY TEA PLATE GREEN</td>\n",
       "      <td>-3</td>\n",
       "      <td>9/30/2011 12:50</td>\n",
       "      <td>1.65</td>\n",
       "      <td>14911.0</td>\n",
       "      <td>Eire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode                         Description  Quantity  \\\n",
       "206139   C554905    15058A        BLUE POLKADOT GARDEN PARASOL        -2   \n",
       "466258   C576324     23470         CARD HOLDER LOVE BIRD LARGE        -1   \n",
       "349892   C567543     23346                     SPACEBOY BEAKER        -6   \n",
       "221643    556260     37340               SOLD AS SET ON DOTCOM       -18   \n",
       "176843   C552029     20982         12 PENCILS TALL TUBE SKULLS        -1   \n",
       "72926    C542273     22099           CARAVAN SQUARE TISSUE BOX        -9   \n",
       "20393     538042     21763            VINTAGE WOODEN BAR STOOL        -4   \n",
       "467904   C576375     22583        PACK OF 6 HANDBAG GIFT BOXES        -1   \n",
       "437426   C574291     22633              HAND WARMER UNION JACK        -1   \n",
       "79477    C542991    85019C         CURIOUS IMAGES NOTEBOOK SET       -12   \n",
       "21473    C538085     82483  WOOD 2 DRAWER CABINET WHITE FINISH        -1   \n",
       "225302   C556647     23081       GREEN METAL BOX ARMY SUPPLIES        -2   \n",
       "198245    554023    82613B      METAL SIGN,CUPCAKE SINGLE HOOK       -71   \n",
       "193448   C553530     22646  CERAMIC STRAWBERRY CAKE MONEY BANK        -5   \n",
       "66000    C541710     21880                  RED RETROSPOT TAPE       -12   \n",
       "422036   C573037     23197   SKETCHBOOK MAGNETIC SHOPPING LIST        -6   \n",
       "419246   C572761     22960            JAM MAKING SET WITH JARS        -3   \n",
       "516815   C579970     22840          ROUND CAKE TIN VINTAGE RED        -2   \n",
       "88062    C543755     21915                RED HARMONICA IN BOX       -12   \n",
       "369950   C569114     23171             REGENCY TEA PLATE GREEN        -3   \n",
       "\n",
       "             InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "206139   5/27/2011 11:17       7.95     14191.0  United Kingdom  \n",
       "466258  11/14/2011 15:10       6.25     16161.0  United Kingdom  \n",
       "349892   9/21/2011 10:19       1.25     13418.0  United Kingdom  \n",
       "221643    6/9/2011 18:02       0.00         NaN  United Kingdom  \n",
       "176843    5/5/2011 18:11       0.85     15622.0  United Kingdom  \n",
       "72926    1/27/2011 10:01       1.25     12578.0           Italy  \n",
       "20393    12/9/2010 13:10       0.00         NaN  United Kingdom  \n",
       "467904   11/15/2011 8:52       2.55     16878.0  United Kingdom  \n",
       "437426   11/3/2011 15:19       2.10     15290.0  United Kingdom  \n",
       "79477     2/2/2011 11:54       4.25     14460.0  United Kingdom  \n",
       "21473    12/9/2010 14:43       5.95     16350.0  United Kingdom  \n",
       "225302   6/13/2011 16:14       8.25     13012.0  United Kingdom  \n",
       "198245   5/20/2011 13:20       0.00         NaN  United Kingdom  \n",
       "193448   5/17/2011 15:08       1.24     13777.0  United Kingdom  \n",
       "66000    1/21/2011 11:17       0.65     14320.0  United Kingdom  \n",
       "422036  10/27/2011 13:45       1.45     12471.0         Germany  \n",
       "419246   10/26/2011 9:38       4.25     12584.0           Italy  \n",
       "516815   12/1/2011 11:13       7.95     14388.0  United Kingdom  \n",
       "88062    2/11/2011 14:48       1.25     14304.0  United Kingdom  \n",
       "369950   9/30/2011 12:50       1.65     14911.0            Eire  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at the negative values in Quantity and look for patterns\n",
    "df_clean[df_clean['Quantity'] < 0].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice some patterns:\n",
    "1. Most of the InvoiceNo values have a C at the beginning, probably meaning they are \"Credit\"\n",
    "2. Some Description values are special cases, like \"DAMAGED\", \"DISCOUNT\", or \"?\"\n",
    "\n",
    "Because of that, let's see how many unique descriptions are associated with negative quantities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2471\n"
     ]
    }
   ],
   "source": [
    "negative_descriptions = df_clean[df_clean['Quantity'] < 0]['Description'].value_counts()\n",
    "print(negative_descriptions.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many negative unique values, we can look for the most frequently used ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description\n",
      "MANUAL                                 244\n",
      "REGENCY CAKESTAND 3 TIER               180\n",
      "POSTAGE                                126\n",
      "CHECK                                  123\n",
      "UNKNOWN                                 97\n",
      "JAM MAKING SET WITH JARS                87\n",
      "DISCOUNT                                77\n",
      "SET OF 3 CAKE TINS PANTRY DESIGN        75\n",
      "SAMPLES                                 61\n",
      "DAMAGED                                 57\n",
      "STRAWBERRY CERAMIC TRINKET BOX          54\n",
      "ROSES REGENCY TEACUP AND SAUCER         54\n",
      "RECIPE BOX PANTRY YELLOW DESIGN         47\n",
      "DAMAGES                                 46\n",
      "JUMBO BAG RED RETROSPOT                 44\n",
      "LUNCH BAG RED RETROSPOT                 44\n",
      "WOOD 2 DRAWER CABINET WHITE FINISH      43\n",
      "RED RETROSPOT CAKE STAND                42\n",
      "WHITE HANGING HEART T-LIGHT HOLDER      42\n",
      "GREEN REGENCY TEACUP AND SAUCER         42\n",
      "?                                       42\n",
      "SMALL GLASS HEART TRINKET POT           40\n",
      "SET OF 3 REGENCY CAKE TINS              37\n",
      "POPCORN HOLDER                          36\n",
      "PINK REGENCY TEACUP AND SAUCER          36\n",
      "SET OF TEA COFFEE SUGAR TINS PANTRY     35\n",
      "SILVER HANGING T-LIGHT HOLDER           35\n",
      "CLASSIC GLASS COOKIE JAR                34\n",
      "SET/5 RED RETROSPOT LID GLASS BOWLS     34\n",
      "AMAZON FEE                              32\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(negative_descriptions.head(30))  # Show the top 30 most frequent negative descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling the negative values in `Quantity`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that some `Description` values represent regular products, while others indicate special cases (discounts, damaged products, or ambiguous values like \"?\"). We implement a unified classification system:\n",
    "\n",
    "1. Categorize All Transactions\n",
    "\n",
    "    Create a `TransactionType` column with three distinct labels:\n",
    "\n",
    "    * **Return:** Transactions where `InvoiceNo` starts with \"C\" (credit notes).\n",
    "    * **SpecialCase:** Transactions with descriptions matching predefined non-product terms (DISCOUNT, DAMAGED, SAMPLES, ?, etc.).\n",
    "    * **Sale:** All other regular transactions.\n",
    "\n",
    "2. Process Negative Quantities\n",
    "\n",
    "    * **Returns:** Keep negatives (valid refund records).\n",
    "    * **Special Cases:** Preserve original values (context-dependent).\n",
    "    * **Sales:** Convert negatives to positives (assumed data entry errors).\n",
    "\n",
    "**Note:** To distinguish legitimate negative quantities from data entry errors:\n",
    "\n",
    "* Identified the top 30 most frequent descriptions for negative quantities\n",
    "\n",
    "* Manually selected non-product terms (e.g., `DISCOUNT`, `DAMAGED`, `?`)\n",
    "\n",
    "Resulting in the curated special_case_list used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined list of special cases for descriptions\n",
    "special_case_list = [\n",
    "    'DISCOUNT', 'DAMAGED', 'DAMAGES', 'SAMPLES', 'CHECK', 'MANUAL', 'POSTAGE', \n",
    "    'UNKNOWN', '?', 'AMAZON FEE'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_transaction(invoice_no, description):\n",
    "    \"\"\"Classify transaction as 'Return', 'SpecialCase', or 'Sale'.\"\"\"\n",
    "    if str(invoice_no).startswith('C'):\n",
    "        return 'Return'\n",
    "    elif description in special_case_list:\n",
    "        return 'SpecialCase'\n",
    "    else:\n",
    "        return 'Sale'\n",
    "\n",
    "# Use .map() efficiently by applying it on a tuple of (InvoiceNo, Description)\n",
    "df_clean['TransactionType'] = list(map(classify_transaction, df_clean['InvoiceNo'], df_clean['Description']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert negatives to positives ONLY for regular Sales\n",
    "df_clean.loc[\n",
    "    (df_clean['TransactionType'] == 'Sale') & \n",
    "    (df_clean['Quantity'] < 0), \n",
    "    'Quantity'\n",
    "] = df_clean['Quantity'].abs()  # Or: *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify this approach worked as intended, let's see if there are any negative values left that are not returns or special cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(df_clean[\n",
    "    (df_clean['Quantity'] < 0) & \n",
    "    (~df_clean['TransactionType'].isin(['Return', 'SpecialCase']))\n",
    "].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result is 0, it means we correctly handled the negative values and can continue to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analizing and handling the negative values in `UnitPrice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen before, there are just 2 transactions with negative UnitPrice. Therefore, we can search for them directly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "      <th>TransactionType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299983</th>\n",
       "      <td>A563186</td>\n",
       "      <td>B</td>\n",
       "      <td>ADJUST BAD DEBT</td>\n",
       "      <td>1</td>\n",
       "      <td>8/12/2011 14:51</td>\n",
       "      <td>-11062.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299984</th>\n",
       "      <td>A563187</td>\n",
       "      <td>B</td>\n",
       "      <td>ADJUST BAD DEBT</td>\n",
       "      <td>1</td>\n",
       "      <td>8/12/2011 14:52</td>\n",
       "      <td>-11062.06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>Sale</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode      Description  Quantity      InvoiceDate  \\\n",
       "299983   A563186         B  ADJUST BAD DEBT         1  8/12/2011 14:51   \n",
       "299984   A563187         B  ADJUST BAD DEBT         1  8/12/2011 14:52   \n",
       "\n",
       "        UnitPrice  CustomerID         Country TransactionType  \n",
       "299983  -11062.06         NaN  United Kingdom            Sale  \n",
       "299984  -11062.06         NaN  United Kingdom            Sale  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean[df_clean['UnitPrice'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since they are labeled as \"ADJUST BAD DEBT\", these appear to represent financial adjustments rather than product sales.  \n",
    "\n",
    "These transactions are valid records, so we will keep them in the dataset and classify them under \"SpecialCase\" in the `TransactionType` column while preserving their negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure \"ADJUST BAD DEBT\" transactions are marked as SpecialCase\n",
    "df_clean.loc[df_clean['Description'] == 'ADJUST BAD DEBT', 'TransactionType'] = 'SpecialCase'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Outliers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
